{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern Recognition and Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Face Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 15:29:51.924726: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-21 15:29:52.812271: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# importing libraries to load and test the models\n",
    "from sklearn.metrics import accuracy_score,classification_report,ConfusionMatrixDisplay\n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage.feature import hog\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After Feature Extraction we found out that HoG, meaning Histogram of Oriented Gradients, we notice that we get around 70,000 features which is quite bulky for models to handle, so we applied PCA on the HoG features alone and extracted 985 features out of those 70,000 which covers 0.95 variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are loading the dataset\n",
    "Dataset was initally broken into two parts train and test. However, we realised that we had to drop many classes due to lack of sufficient images to train. Thus, we concatenated these two datasets together and had carried out training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset part 01 contains 10,586 records with 3248 features\n",
    "# Dataset part 02 contains 2647 records with 3248 features\n",
    "dataset_part_01 = pd.read_csv(\"\",header = None)\n",
    "dataset_part_02 = pd.read_csv(\"\",header = None)\n",
    "\n",
    "dataset = pd.concat([dataset_part_01,dataset_part_02],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.head())\n",
    "\n",
    "print(dataset.info())\n",
    "\n",
    "print(dataset.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may notice that that features are not in the same range, so, we normalized the data as Zero Mean and One Standard Deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, We load the models trained and tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Face_Recognition:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        # self.image = None\n",
    "        self.__models = []\n",
    "        self.__model_weights = {}\n",
    "        self.scaler=joblib.load('Joblib_files/StandardScaler.joblib')\n",
    "        self.ann_list=['Hugo_Chavez','George_W_Bush','Donald_Rumsfeld', 'Colin_Powell'\n",
    " 'Tony_Blair', 'Junichiro_Koizumi', 'Ariel_Sharon' 'Jean_Chretien'\n",
    " 'Jacques_Chirac', 'Gerhard_Schroeder', 'Serena_Williams', 'John_Ashcroft']\n",
    "\n",
    "    def __compute_hog_w_pca(self,image):\n",
    "        img=imread(image)\n",
    "        resized_img = resize(img, (128, 64))\n",
    "        fd, hog_image = hog(resized_img, orientations=9, pixels_per_cell=(8, 8),\n",
    "                            cells_per_block=(2, 2), visualize=True, channel_axis=-1)\n",
    "        pca_hog=joblib.load('Joblib_files/pca_hog_model.joblib')\n",
    "        fd=fd.reshape(1, -1)\n",
    "        # print(fd.shape)\n",
    "        hfe=pca_hog.transform(fd)\n",
    "        return hfe\n",
    "    \n",
    "    def __compute_cnn_feat(self,image):\n",
    "        image = Image.open(image).convert('RGB')\n",
    "        preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        image = preprocess(image)\n",
    "        \n",
    "        # Add batch dimension\n",
    "        image = image.unsqueeze(0)\n",
    "        # Extract features\n",
    "                    # Load pre-trained ResNet-50 model\n",
    "        resnet = models.resnet50(weights='ResNet50_Weights.DEFAULT')\n",
    "        # Remove the last fully connected layer\n",
    "        resnet = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        # Set the model to evaluation mode\n",
    "        resnet.eval()\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            features = resnet(image)\n",
    "        # Remove the batch dimension\n",
    "        features = features.squeeze(0)\n",
    "        features_array = np.array([round(feature.item(), 4) for feature in features])\n",
    "\n",
    "        return features_array\n",
    "\n",
    "    def __get_pixel(self,img, center, x, y):\n",
    "        new_value = 0\n",
    "        try:\n",
    "            if img[x][y] >= center:\n",
    "                new_value = 1\n",
    "        except:\n",
    "            pass\n",
    "        return new_value\n",
    "    \n",
    "    def __lbp_calculated_pixel(self,img, x, y):\n",
    "        center = img[x][y]\n",
    "        val_ar = []\n",
    "        val_ar.append(self.__get_pixel(img, center, x-1, y-1))     # top_left\n",
    "        val_ar.append(self.__get_pixel(img, center, x, y-1))       # top\n",
    "        val_ar.append(self.__get_pixel(img, center, x+1, y-1))     # top_right\n",
    "        val_ar.append(self.__get_pixel(img, center, x+1, y))       # right\n",
    "        val_ar.append(self.__get_pixel(img, center, x+1, y+1))     # bottom_right\n",
    "        val_ar.append(self.__get_pixel(img, center, x, y+1))       # bottom\n",
    "        val_ar.append(self.__get_pixel(img, center, x-1, y+1))     # bottom_left\n",
    "        val_ar.append(self.__get_pixel(img, center, x-1, y))       # left\n",
    "\n",
    "        power_val = [1, 2, 4, 8, 16, 32, 64, 128] #this depict powers of 2 starting from top_left\n",
    "        val = 0\n",
    "        for i in range(len(val_ar)):\n",
    "            val += val_ar[i] * power_val[i]\n",
    "        return val\n",
    "\n",
    "    def __calcLBP(self,img):\n",
    "        height, width, channel = img.shape\n",
    "        # print(height,width,channel)\n",
    "        img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        img_lbp = np.zeros((height, width,3), np.uint8)\n",
    "        for i in range(0, height):\n",
    "            for j in range(0, width):\n",
    "                img_lbp[i, j] = self.__lbp_calculated_pixel(img_gray, i, j)\n",
    "        hist_lbp = cv2.calcHist([img_lbp], [0], None, [256], [0, 256])  \n",
    "        hist_lbp=hist_lbp.flatten()\n",
    "        return hist_lbp\n",
    "\n",
    "  \n",
    "    def __lbp_feat(self,image):\n",
    "        img=cv2.imread(image)\n",
    "        features_lbp=self.__calcLBP(img)\n",
    "        return features_lbp\n",
    "\n",
    "\n",
    "    def __concatenate_features(self,image):\n",
    "        hog_features=self.__compute_hog_w_pca(image)\n",
    "        cnn_features=self.__compute_cnn_feat(image)\n",
    "        lbp_features=self.__lbp_feat(image)\n",
    "        hog_features=hog_features.reshape(-1)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        conc_array = np.concatenate((hog_features, cnn_features, lbp_features))\n",
    "        conc_array_norm=self.scaler.transform(conc_array.reshape(1,-1))\n",
    "\n",
    "        return conc_array_norm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __loadModels (self):\n",
    "        # We are loading the hyper parameter tuned models that were saved using joblib\n",
    "\n",
    "        model_Decision_Tree = joblib.load(\"Joblib_files/tuned_dt_conc_50_gd.joblib\")\n",
    "        self.__models.append(model_Decision_Tree)\n",
    "        self.__model_weights[model_Decision_Tree] = 10\n",
    "\n",
    "        model_Random_Forest = joblib.load(\"Joblib_files/model_all_Random_Forest.joblib\")\n",
    "        self.__models.append(model_Random_Forest)\n",
    "        self.__model_weights[model_Random_Forest] = 10\n",
    "\n",
    "\n",
    "        model_SVM = joblib.load(\"Joblib_files/LinearSVC_All_n.joblib\")\n",
    "        self.__models.append(model_SVM)\n",
    "        self.__model_weights[model_SVM] = 10\n",
    "\n",
    "\n",
    "        model_Naive_Bayes = joblib.load(\"Joblib_files/gaussian.joblib\")\n",
    "        self.__models.append(model_Naive_Bayes)\n",
    "        self.__model_weights[model_Naive_Bayes] = 10\n",
    "\n",
    "        # model_kNN = joblib.load(\"Joblib_files/all_knn.joblib\")\n",
    "        # self.__models.append(model_kNN)\n",
    "        # self.__model_weights[model_kNN] = 10\n",
    "\n",
    "    \n",
    "\n",
    "        model_ANN = load_model(\"Decision_tree/saved_models/ann_50.h5\")\n",
    "        self.__models.append(model_ANN)\n",
    "        self.__model_weights[model_ANN] = 10\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "    def __printModel_Parameters (self):\n",
    "        # Printing the models with their Hyper Parameters\n",
    "\n",
    "        print(\"Decision Tree \",self.__models[0])\n",
    "\n",
    "        print(\"Random Forest \",self.__models[1])\n",
    "\n",
    "        print(\"Support Vector Machine \",self.__models[2])\n",
    "\n",
    "        print(\"Naive Bayes \",self.__models[3])\n",
    "\n",
    "        # print(\"kNN \",self.__models[4])\n",
    "\n",
    "        print(\"Artificial Neural Network \",self.__models[4])\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "    def predict(self,image):\n",
    "\n",
    "        ## Call the feature extraction function and store it in the variable X ##\n",
    "\n",
    "        ## Set up the model weights ##\n",
    "\n",
    "        ## Predictions are stored here\n",
    "        y_predictions = []\n",
    "        X=self.__concatenate_features(image)\n",
    "\n",
    "        self.__loadModels()\n",
    "        self.__printModel_Parameters()\n",
    "\n",
    "        # Receive prediction of each model\n",
    "        for model in self.__models:\n",
    "\n",
    "            model_prediction = model.predict(X)\n",
    "\n",
    "            # Add the prediction of model the weight number of times to get better performance\n",
    "\n",
    "            for iter in range(self.__model_weights[model]):\n",
    "                if model == self.__models[-1]:  # Check if the model is the ANN\n",
    "                    ann_index = int(np.argmax(model_prediction))  # Assuming model_prediction is the index\n",
    "                    print(ann_index)\n",
    "                    print(self.ann_list)\n",
    "                    ann_prediction = self.ann_list[ann_index]\n",
    "                    y_predictions.append(ann_prediction)\n",
    "                else:\n",
    "                    y_predictions.append(model_prediction)\n",
    "        \n",
    "\n",
    "        # Get the count that has max frequency and return that prediction\n",
    "        max_count = 0\n",
    "        predicted_class = ''\n",
    "\n",
    "        for element in y_predictions:\n",
    "\n",
    "            count = y_predictions.count(element)\n",
    "            if y_predictions.count(element) > max_count:\n",
    "                max_count = count\n",
    "                predicted_class = element\n",
    "            \n",
    "        return predicted_class\n",
    "    \n",
    "    def __kl_divergence(self,p, q):\n",
    "        # Clip values to avoid log(0) and log(1)\n",
    "        p = np.clip(p, 1e-10, 1 - 1e-10)\n",
    "        q = np.clip(q, 1e-10, 1 - 1e-10)\n",
    "        \n",
    "        # Compute KL divergence\n",
    "        kl_div = np.sum(p * np.log(p / q))\n",
    "        \n",
    "        return kl_div\n",
    "    \n",
    "    def get_similarity (self,image1,image2,model):\n",
    "\n",
    "        ### Write the code here ###\n",
    "        test1=self.__concatenate_features(image1)\n",
    "        test2=self.__concatenate_features(image2)\n",
    "\n",
    "        \n",
    "\n",
    "        arr_1= test1.reshape(1, -1)\n",
    "        \n",
    "        arr_2= test2.reshape(1, -1)\n",
    "\n",
    "        y1=model.predict(arr_1)\n",
    "        y2=model.predict(arr_2)\n",
    "\n",
    "        # Compute the argmax for both predictions\n",
    "        argmax1 = np.argmax(y1)\n",
    "        argmax2 = np.argmax(y2)\n",
    "        \n",
    "        # getting the label\n",
    "        # y_pred = label_encoder.inverse_transform(np.argmax(y1, axis=1))\n",
    "        # print(y_pred)\n",
    "        print(argmax1)\n",
    "        # print(y1.shape)\n",
    "        # print(y2.shape)\n",
    "\n",
    "        # If the argmax values are the same\n",
    "        if argmax1 == argmax2:\n",
    "            # Compute the mean of the probabilities for each class\n",
    "            mean_probs = (y1 + y2) / 2\n",
    "            return np.max(mean_probs), \"Same\"\n",
    "    \n",
    "        # If the argmax values are different\n",
    "        else:\n",
    "            kl_div = self.__kl_divergence(y1.flatten(), y2.flatten())\n",
    "\n",
    "            return kl_div, \"Different\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adi_techbuddy/anaconda3/envs/tf/lib/python3.8/site-packages/sklearn/base.py:347: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.4.2 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/adi_techbuddy/anaconda3/envs/tf/lib/python3.8/site-packages/sklearn/base.py:347: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.4.2 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/adi_techbuddy/anaconda3/envs/tf/lib/python3.8/site-packages/sklearn/base.py:347: InconsistentVersionWarning: Trying to unpickle estimator RandomForestClassifier from version 1.4.2 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/adi_techbuddy/anaconda3/envs/tf/lib/python3.8/site-packages/sklearn/base.py:347: InconsistentVersionWarning: Trying to unpickle estimator LinearSVC from version 1.4.2 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/adi_techbuddy/anaconda3/envs/tf/lib/python3.8/site-packages/sklearn/base.py:347: InconsistentVersionWarning: Trying to unpickle estimator GaussianNB from version 1.4.2 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree  DecisionTreeClassifier(max_depth=8, max_leaf_nodes=40, min_samples_leaf=10,\n",
      "                       min_samples_split=60, random_state=42)\n",
      "Random Forest  RandomForestClassifier(max_depth=2500, max_leaf_nodes=500, min_samples_leaf=2,\n",
      "                       n_estimators=95)\n",
      "Support Vector Machine  LinearSVC(dual=False)\n",
      "Naive Bayes  GaussianNB()\n",
      "Artificial Neural Network  <keras.src.engine.sequential.Sequential object at 0x761ab43b0d30>\n",
      "1/1 [==============================] - 0s 157ms/step\n",
      "3\n",
      "['Hugo_Chavez', 'George_W_Bush', 'Donald_Rumsfeld', 'Colin_PowellTony_Blair', 'Junichiro_Koizumi', 'Ariel_SharonJean_ChretienJacques_Chirac', 'Gerhard_Schroeder', 'Serena_Williams', 'John_Ashcroft']\n",
      "3\n",
      "['Hugo_Chavez', 'George_W_Bush', 'Donald_Rumsfeld', 'Colin_PowellTony_Blair', 'Junichiro_Koizumi', 'Ariel_SharonJean_ChretienJacques_Chirac', 'Gerhard_Schroeder', 'Serena_Williams', 'John_Ashcroft']\n",
      "3\n",
      "['Hugo_Chavez', 'George_W_Bush', 'Donald_Rumsfeld', 'Colin_PowellTony_Blair', 'Junichiro_Koizumi', 'Ariel_SharonJean_ChretienJacques_Chirac', 'Gerhard_Schroeder', 'Serena_Williams', 'John_Ashcroft']\n",
      "3\n",
      "['Hugo_Chavez', 'George_W_Bush', 'Donald_Rumsfeld', 'Colin_PowellTony_Blair', 'Junichiro_Koizumi', 'Ariel_SharonJean_ChretienJacques_Chirac', 'Gerhard_Schroeder', 'Serena_Williams', 'John_Ashcroft']\n",
      "3\n",
      "['Hugo_Chavez', 'George_W_Bush', 'Donald_Rumsfeld', 'Colin_PowellTony_Blair', 'Junichiro_Koizumi', 'Ariel_SharonJean_ChretienJacques_Chirac', 'Gerhard_Schroeder', 'Serena_Williams', 'John_Ashcroft']\n",
      "3\n",
      "['Hugo_Chavez', 'George_W_Bush', 'Donald_Rumsfeld', 'Colin_PowellTony_Blair', 'Junichiro_Koizumi', 'Ariel_SharonJean_ChretienJacques_Chirac', 'Gerhard_Schroeder', 'Serena_Williams', 'John_Ashcroft']\n",
      "3\n",
      "['Hugo_Chavez', 'George_W_Bush', 'Donald_Rumsfeld', 'Colin_PowellTony_Blair', 'Junichiro_Koizumi', 'Ariel_SharonJean_ChretienJacques_Chirac', 'Gerhard_Schroeder', 'Serena_Williams', 'John_Ashcroft']\n",
      "3\n",
      "['Hugo_Chavez', 'George_W_Bush', 'Donald_Rumsfeld', 'Colin_PowellTony_Blair', 'Junichiro_Koizumi', 'Ariel_SharonJean_ChretienJacques_Chirac', 'Gerhard_Schroeder', 'Serena_Williams', 'John_Ashcroft']\n",
      "3\n",
      "['Hugo_Chavez', 'George_W_Bush', 'Donald_Rumsfeld', 'Colin_PowellTony_Blair', 'Junichiro_Koizumi', 'Ariel_SharonJean_ChretienJacques_Chirac', 'Gerhard_Schroeder', 'Serena_Williams', 'John_Ashcroft']\n",
      "3\n",
      "['Hugo_Chavez', 'George_W_Bush', 'Donald_Rumsfeld', 'Colin_PowellTony_Blair', 'Junichiro_Koizumi', 'Ariel_SharonJean_ChretienJacques_Chirac', 'Gerhard_Schroeder', 'Serena_Williams', 'John_Ashcroft']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['George_W_Bush'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faceRecognition = Face_Recognition()\n",
    "img1='image4.jpg'\n",
    "img2='image2.jpg'\n",
    "\n",
    "faceRecognition.predict(img1)\n",
    "\n",
    "\n",
    "\n",
    "# model=load_model('Decision_tree/saved_models/ann_50.h5')\n",
    "# faceRecognition.get_similarity(img1,img2,model)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
