{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage.io import imread\n",
    "from skimage.feature import hog,local_binary_pattern\n",
    "from skimage.feature import local_binary_pattern\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfw_allnames = pd.read_csv(\"lfw_allnames.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_w_path = lfw_allnames.loc[lfw_allnames.index.repeat(lfw_allnames['images'])]  #repeat till how many images are there \n",
    "data_w_path = data_w_path.drop(\"images\",axis=1)  #remove the image total count column\n",
    "data_w_path['image_path'] = 1 + data_w_path.groupby('name').cumcount()\n",
    "data_w_path['image_path'] = data_w_path.image_path.apply(lambda x: '{0:0>4}'.format(x))\n",
    "data_w_path['image_path'] = data_w_path.name + \"/\" + data_w_path.name + \"_\" + data_w_path.image_path + \".jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_w_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_w_path['name'].value_counts()[:10].plot(kind = \"bar\")  #first 20 in ascending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting dataset\n",
    "lfw_train, lfw_test = train_test_split(data_w_path, test_size=0.2)\n",
    "lfw_train = lfw_train.reset_index().drop(\"index\",axis=1)  #resetting the index and dropping the column names index\n",
    "lfw_test = lfw_test.reset_index().drop(\"index\",axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lfw_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verifying the similarities between train and test\n",
    "print(len(set(lfw_train.name).intersection(set(lfw_test.name))))  #total number of same names in train and test\n",
    "print(len(set(lfw_test.name) - set(lfw_train.name)))  #names which occurred in test but not in train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open(\"dataset\" + str(lfw_train.image_path[0]))\n",
    "plt.imshow(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>HOG_95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"dataset\"\n",
    "\n",
    "\n",
    "# Define a function to compute HOG features for an image\n",
    "def compute_hog(img):\n",
    "    # Resizing image\n",
    "    resized_img = resize(img, (128, 64))\n",
    "    # Creating HOG features\n",
    "    fd, hog_image = hog(resized_img, orientations=9, pixels_per_cell=(8, 8),\n",
    "                    cells_per_block=(2, 2), visualize=True, channel_axis=-1)\n",
    "    return fd\n",
    "\n",
    "# Assuming lfw_test is a DataFrame with 'image_path' column containing paths to images\n",
    "# Loop through each row in lfw_test\n",
    "hog_features = []\n",
    "for index, row in lfw_train.iterrows():\n",
    "    path = row['image_path']\n",
    "\n",
    "    image_path=os.path.join(folder,path)\n",
    "    img = imread(image_path)\n",
    "\n",
    "    # Computing HOG features for the image\n",
    "    features_hog = compute_hog(img)\n",
    "    hog_features.append(features_hog)\n",
    "\n",
    "# Converting hog_features list to DataFrame\n",
    "hog_features_df = pd.DataFrame(hog_features)\n",
    "\n",
    "# Apply PCA to reduce dimensionality while retaining 0.95 variance\n",
    "pca_hog = PCA(n_components=0.95)\n",
    "pca_result_hog = pca_hog.fit_transform(hog_features_df)\n",
    "\n",
    "# Now pca_result contains the reduced dimensional features with 0.95 variance\n",
    "print(\"Original number of features:\", hog_features_df.shape[1])\n",
    "print(\"Reduced number of features after PCA:\", pca_result_hog.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline for all functions for extracting features\n",
    "\n",
    "# Load pre-trained ResNet-50 model\n",
    "resnet = models.resnet50(weights='ResNet50_Weights.DEFAULT')\n",
    "# Remove the last fully connected layer\n",
    "resnet = nn.Sequential(*list(resnet.children())[:-1])\n",
    "# Set the model to evaluation mode\n",
    "resnet.eval()\n",
    "\n",
    "# Define a function to extract features from an image\n",
    "def extract_features(image_path, model):\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    # plt.imshow(image)\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    image = preprocess(image)\n",
    "    \n",
    "    # Add batch dimension\n",
    "    image = image.unsqueeze(0)\n",
    "    # Extract features\n",
    "    with torch.no_grad():\n",
    "        features = model(image)\n",
    "    # Remove the batch dimension\n",
    "    features = features.squeeze(0)\n",
    "    features_array = np.array([round(feature.item(), 4) for feature in features])\n",
    "\n",
    "    return features_array\n",
    "\n",
    "\n",
    "cnn_features = []\n",
    "for index, row in lfw_train.iterrows():\n",
    "    path = row['image_path']\n",
    "#\n",
    "    image_path=os.path.join(folder,path)\n",
    "    features_cnn = extract_features(image_path, resnet)  #for cnn features\n",
    "    cnn_features.append(features_cnn)\n",
    "\n",
    "cnn_array=np.array(cnn_features)\n",
    "\n",
    "\n",
    "# for applying pca on cnn features\n",
    "# # Convert cnn_features list to DataFrame\n",
    "# cnn_features_df = pd.DataFrame(cnn_features)\n",
    "\n",
    "# # Apply PCA to reduce dimensionality while retaining 0.95 variance\n",
    "# pca_cnn = PCA(n_components=0.95)\n",
    "# pca_result_cnn = pca_cnn.fit_transform(cnn_features_df)\n",
    "\n",
    "# # Now pca_result contains the reduced dimensional features with 0.95 variance\n",
    "# print(\"Original number of features:\", cnn_features_df.shape[1])\n",
    "# print(\"Reduced number of features after PCA:\", pca_result_cnn.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixel(img, center, x, y):\n",
    "    new_value = 0\n",
    "    try:\n",
    "        if img[x][y] >= center:\n",
    "            new_value = 1\n",
    "    except:\n",
    "        pass\n",
    "    return new_value\n",
    "\n",
    "def lbp_calculated_pixel(img, x, y):\n",
    "    center = img[x][y]\n",
    "    val_ar = []\n",
    "    val_ar.append(get_pixel(img, center, x-1, y-1))     # top_left\n",
    "    val_ar.append(get_pixel(img, center, x, y-1))       # top\n",
    "    val_ar.append(get_pixel(img, center, x+1, y-1))     # top_right\n",
    "    val_ar.append(get_pixel(img, center, x+1, y))       # right\n",
    "    val_ar.append(get_pixel(img, center, x+1, y+1))     # bottom_right\n",
    "    val_ar.append(get_pixel(img, center, x, y+1))       # bottom\n",
    "    val_ar.append(get_pixel(img, center, x-1, y+1))     # bottom_left\n",
    "    val_ar.append(get_pixel(img, center, x-1, y))       # left\n",
    "\n",
    "    power_val = [1, 2, 4, 8, 16, 32, 64, 128] #this depict powers of 2 starting from top_left\n",
    "    val = 0\n",
    "    for i in range(len(val_ar)):\n",
    "        val += val_ar[i] * power_val[i]\n",
    "    return val\n",
    "\n",
    "\n",
    "def calcLBP(img):\n",
    "    height, width, channel = img.shape\n",
    "    # print(height,width,channel)\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    img_lbp = np.zeros((height, width,3), np.uint8)\n",
    "    for i in range(0, height):\n",
    "        for j in range(0, width):\n",
    "             img_lbp[i, j] = lbp_calculated_pixel(img_gray, i, j)\n",
    "    hist_lbp = cv2.calcHist([img_lbp], [0], None, [256], [0, 256])  \n",
    "    hist_lbp=hist_lbp.flatten()\n",
    "    return hist_lbp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbp_features = []\n",
    "for index, row in lfw_train.iterrows():\n",
    "        path = row['image_path']\n",
    "        image_path = os.path.join(folder, path)\n",
    "        img2=cv2.imread(image_path)\n",
    "        features_lbp=calcLBP(img2)\n",
    "        lbp_features.append(features_lbp)\n",
    "\n",
    "lbp_features_array = np.array(lbp_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbp_features_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# giving labels to numpy array \n",
    "labels=lfw_train['name'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating the arrays along the columns (axis=1)\n",
    "concatenated_array = np.concatenate((pca_result_hog, cnn_array, lbp_features_array), axis=1)\n",
    "\n",
    "# Combine concatenated features with labels\n",
    "data_with_labels = np.column_stack((labels, concatenated_array))\n",
    "\n",
    "# Save the combined data as a CSV file\n",
    "np.savetxt('extracted_features_hog_95.csv', data_with_labels, delimiter=',', fmt='%s')\n",
    "\n",
    "print(\"Extraction complete. Data saved to: extracted_features_hog_95.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verifying that csv is created properly\n",
    "df = pd.read_csv(\"extracted_features_hog_95.csv\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Test data\n",
    "hog_features_test = []\n",
    "for index, row in lfw_test.iterrows():\n",
    "    path = row['image_path']\n",
    "    image_path=os.path.join(folder,path)\n",
    "    img = imread(image_path)\n",
    "    # Compute HOG features for the image\n",
    "    features_hog = compute_hog(img)\n",
    "    hog_features_test.append(features_hog)\n",
    "\n",
    "\n",
    "\n",
    "hog_features_test_df = pd.DataFrame(hog_features_test)\n",
    "\n",
    "# Apply PCA to reduce dimensionality while retaining 0.95 variance\n",
    "\n",
    "pca_result_hog_test = pca_hog.transform(hog_features_test_df)  #using the same pca trained on training data so as to get the same number of features\n",
    "\n",
    "# Now pca_result contains the reduced dimensional features with 0.95 variance\n",
    "print(\"Original number of features:\", hog_features_test_df.shape[1])\n",
    "print(\"Reduced number of features after PCA:\", pca_result_hog_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_features_test = []\n",
    "for index, row in lfw_test.iterrows():\n",
    "    path = row['image_path']\n",
    "#\n",
    "    image_path=os.path.join(folder,path)\n",
    "    features_cnn = extract_features(image_path, resnet)  #for cnn features\n",
    "    cnn_features_test.append(features_cnn)\n",
    "\n",
    "\n",
    "cnn_array_test=np.array(cnn_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbp_features_test = []\n",
    "for index, row in lfw_test.iterrows():\n",
    "        path = row['image_path']\n",
    "        image_path = os.path.join(folder, path)\n",
    "        img2=cv2.imread(image_path)\n",
    "        features_lbp=calcLBP(img2)\n",
    "        lbp_features_test.append(features_lbp)\n",
    "\n",
    "        \n",
    "lbp_features_array_test = np.array(lbp_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test=lfw_test['name'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the arrays along the columns (axis=1)\n",
    "concatenated_array_test = np.concatenate((pca_result_hog_test, cnn_array_test, lbp_features_array_test), axis=1)\n",
    "\n",
    "# Combine concatenated features with labels\n",
    "data_with_labels_test = np.column_stack((labels_test, concatenated_array_test))\n",
    "\n",
    "# Save the combined data as a CSV file\n",
    "np.savetxt('extracted_features_test_hog_95.csv', data_with_labels_test, delimiter=',', fmt='%s')\n",
    "\n",
    "print(\"Extraction complete. Data saved to: extracted_features_test_hog95.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
